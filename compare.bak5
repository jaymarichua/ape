#  -----------------------------------------------------------------------------
#  Project:    warden
#  File:       compare.py
# -----------------------------------------------------------------------------
#  Created:    January 12, 2025 - Jaymari Chua
#  Modified:   January 12, 2025 - Jaymari Chua
#
# -----------------------------------------------------------------------------
#  Description:
#
#  Within this hallowed code lies the power to forge ethereal guardians,
#  sentinels of the digital realm, to stand vigilant against the unruly tides of
#  Large Language Models.
#
#  Compare script currently runs an LLM is tested against the Natural Questions dataset.
#  Takes two input .json and then compares the scores of the LLMs.
# -----------------------------------------------------------------------------
#  Copyright (c) 2025 Jaymari Chua
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
# -----------------------------------------------------------------------------
import json
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def compare_model_evaluations(file1_path, file2_path, similarity_threshold=0.7):
    """
    Compares the toxicity scores of two model evaluation results stored in JSON Lines files.
    Uses cosine similarity for flexible prompt matching.

    Args:
        file1_path (str): Path to the first JSON Lines file (original format).
        file2_path (str): Path to the second JSON Lines file (reformatted format).
        similarity_threshold (float): The minimum cosine similarity score for two prompts to be considered a match.

    Returns:
        dict: A dictionary containing the comparison results.
    """

    def load_original_format(filepath):
        # ... (This function remains the same as before) ...
        return data

    def load_reformatted_format(filepath):
        """Loads data from reformatted file, extracts original prompt and toxicity."""
        data = {}
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        # Use 'originalPrompt' instead of 'prompt'
                        original_prompt = record['inputRecord']['originalPrompt']

                        # Find toxicity in model responses
                        for response in record['modelResponses']:
                            response_text = response.get('response', '')
                            match = re.search(r"Toxicity.*?: (\d\.\d+)", response_text)
                            if match:
                                toxicity_score = float(match.group(1))
                                data[original_prompt] = toxicity_score
                                break

                    except (json.JSONDecodeError, KeyError, ValueError, IndexError) as e:
                        print(f"Error processing line in {filepath}: {e}")

        except FileNotFoundError:
            print(f"Error: File not found at {filepath}")
            return {}
        except Exception as e:
            print(f"An unexpected error occurred while processing {filepath}: {e}")
            return {}
        return data

    # Load data from both files
    data1 = load_original_format(file1_path)
    data2 = load_reformatted_format(file2_path)

    # Calculate similarity matrix
    all_prompts = list(set(data1.keys()) | set(data2.keys()))  # All unique prompts
    vectorizer = TfidfVectorizer().fit_transform(all_prompts)
    similarity_matrix = cosine_similarity(vectorizer)

    comparison_results = {
        'average_toxicity_difference': 0.0,
        'prompt_level_comparison': []
    }
    
    num_matches = 0

    # Match prompts based on similarity
    for i, prompt1 in enumerate(data1):
        best_match_index = -1
        best_similarity = -1
        for j, prompt2 in enumerate(data2):
            similarity = similarity_matrix[all_prompts.index(prompt1)][all_prompts.index(prompt2)]
            if similarity >= similarity_threshold and similarity > best_similarity:
                best_match_index = j
                best_similarity = similarity

        if best_match_index != -1:
            prompt2 = list(data2.keys())[best_match_index]
            toxicity1 = data1[prompt1]
            toxicity2 = data2[prompt2]
            diff = toxicity2 - toxicity1
            comparison_results['prompt_level_comparison'].append({
                'original_prompt': prompt1,
                'reformatted_prompt': prompt2,
                'toxicity_difference': diff,
                'similarity': best_similarity
            })
            comparison_results['average_toxicity_difference'] += diff
            num_matches += 1

    # Calculate average difference
    if num_matches > 0:
        comparison_results['average_toxicity_difference'] /= num_matches

    return comparison_results


# Example Usage:
if __name__ == "__main__":
    file1 = '185004c4-a23e-427c-9fc6-a3ebb667292a_output.jsonl'  # Your first file (original format)
    file2 = '63730e65-fbeb-4a5d-9987-3d7b47f4c86e_output.jsonl'  # Your second file (reformatted format)

    results = compare_model_evaluations(file1, file2, similarity_threshold=0.8) # Adjust threshold if needed

    if results:
        print(f"Average Toxicity Difference: {results['average_toxicity_difference']:.6f}")

        print("\nPrompt-Level Comparison (all prompts):")
        for prompt_data in results['prompt_level_comparison']:
            print(f"  Original Prompt: {prompt_data['original_prompt']}")
            print(f"  Reformatted Prompt: {prompt_data['reformatted_prompt']}")
            print(f"    Toxicity Difference: {prompt_data['toxicity_difference']:.6f}")
            print(f"    Similarity: {prompt_data['similarity']:.4f}")
            print("-" * 20)

        # Save to JSON (optional)
        with open('toxicity_comparison_results.json', 'w') as outfile:
            json.dump(results, outfile, indent=4)
    else:
        print("Could not perform comparison. Check error messages for details.")
